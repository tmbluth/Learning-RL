{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "How can you know the fastest way to get from point A to point B? Well you need to know the distance between points, of course. But what if there are multiple non-linear ways to get there? What then?\n",
    "\n",
    "This is where experience comes in. Since we have taken certain paths to get from one point to another we know which path is the best. This is only discovered once we:\n",
    "1) reach the end of the trip and\n",
    "2) have tried multiple trip paths to know the best one.\n",
    "\n",
    "Machines can learn the same way! We can provide an agent its environment, tell it where to start, where to end, and where to avoid. The agent will be rewarded only when it reaches its destination. Just like humans don't like waiting in traffic or taking all day to get somewhere, we can discount its final reward by diminishing its reward over time/different states. This will \"motivate\" the agent to pick routes that took as little time as possible. \n",
    "\n",
    "I'll set up a deterministic discrete 4 x 3 environment where there are states and actions that occur in turns. It will look like this:\n",
    "\n",
    "-----------------\n",
    "   |   |   | +1  \n",
    "-----------------\n",
    "   | X |   | -1 \n",
    "-----------------\n",
    " A |   |   |    \n",
    "-----------------\n",
    "\n",
    "...where 'A' is the agent's starting point, 'X' is a state that cannot be reached and the numbers are the reward states. \n",
    "\n",
    "The agent will make an policy of moves it should take only once it reaches a terminal state, which in this case is 1 or -1. Once that maximum reward is found it will look back on which moves it took to get to that point. The values of each state will be the terminal state (either 1 or -1) + the value of the previous state + the value of the previous state, etc. Since the value of each state will be 0 the end result will just be 1, which is problematic if our agent is trying to maximize its reward. Why couldn't it just wander for a long period of time before transitioning to the +1 state and call THAT its optimal policy? This is where the discounting hyperparameter comes in.\n",
    "\n",
    "If we can discount that reward of 1 with -0.1 each time the agent makes a transition between states, then it will want to make as few moves as possible to to maximize its reward. Once it does get to the end, it looks back at the moves it took to get there and calculates the value. So if it took 10 moves, the reward would be 0. Then it tries again. If the second round only took 6 moves, then the reward is 0.4 which is greater. The process repeats until the gain in reward no longer improves. Let's try it out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note: you may need to update your version of future\n",
    "# sudo pip install -U future\n",
    "from __future__ import print_function, division\n",
    "from builtins import range\n",
    "import numpy as np\n",
    "\n",
    "class Grid: # Environment\n",
    "    def __init__(self, width, height, start):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.i = start[0]\n",
    "        self.j = start[1]\n",
    "    \n",
    "    # This function makes rewards and actions per state into class attributes but is added to the class once they are specified\n",
    "    def set(self, rewards, actions):\n",
    "        # rewards should be a dict of: (i, j): r (row, col): reward\n",
    "        # actions should be a dict of: (i, j): A (row, col): list of possible actions\n",
    "        self.rewards = rewards\n",
    "        self.actions = actions\n",
    "    \n",
    "    # Takes a tuple of a state and assigns each x and y index to i and j\n",
    "    def set_state(self, s):\n",
    "        self.i = s[0]\n",
    "        self.j = s[1]\n",
    "    \n",
    "    # Returns tuple of current state\n",
    "    def current_state(self):\n",
    "        return (self.i, self.j)\n",
    "    \n",
    "    # Checks if agent has reached a terminal state\n",
    "    def is_terminal(self, s):\n",
    "        return s not in self.actions\n",
    "    \n",
    "    # How the agent interacts with the environment\n",
    "    def move(self, action):\n",
    "        # Check if legal move first\n",
    "        if action in self.actions[(self.i, self.j)]:\n",
    "            if action == 'U':\n",
    "                self.i -= 1\n",
    "            elif action == 'D':\n",
    "                self.i += 1\n",
    "            elif action == 'R':\n",
    "                self.j += 1\n",
    "            elif action == 'L':\n",
    "                self.j -= 1\n",
    "        # Return a reward from that state if any. If none, return 0\n",
    "        return self.rewards.get((self.i, self.j), 0)\n",
    "    \n",
    "    # Used to backtrack as the optimal policy is calculated\n",
    "    def undo_move(self, action):\n",
    "        # These are the opposite of what U/D/L/R should normally do\n",
    "        if action == 'U':\n",
    "            self.i += 1\n",
    "        elif action == 'D':\n",
    "            self.i -= 1\n",
    "        elif action == 'R':\n",
    "            self.j -= 1\n",
    "        elif action == 'L':\n",
    "            self.j += 1\n",
    "        # Raise an exception if we arrive somewhere we shouldn't be\n",
    "        assert(self.current_state() in self.all_states())\n",
    "    \n",
    "    # Returns true if game is over, else false\n",
    "    def game_over(self):\n",
    "        # true if we are in a state where no actions are possible\n",
    "        return (self.i, self.j) not in self.actions\n",
    "    \n",
    "    # Simple way to get all states. ***Possibly buggy***\n",
    "    def all_states(self):\n",
    "        # Either a position that has possible next actions or a position that yields a reward\n",
    "        return set(self.actions.keys()) | set(self.rewards.keys())\n",
    "\n",
    "# Define a grid that describes the reward for arriving at each state and possible actions at each state\n",
    "def standard_grid():\n",
    "    g = Grid(3, 4, (2, 0))\n",
    "    # 'rewards' disctionary has states as keys and consequnces as values\n",
    "    rewards = {(0, 3): 1, (1, 3): -1}\n",
    "    # 'actions' dictionary has states as keys and possible actions as values\n",
    "    actions = {\n",
    "    (0, 0): ('D', 'R'),\n",
    "    (0, 1): ('L', 'R'),\n",
    "    (0, 2): ('L', 'D', 'R'),\n",
    "    (1, 0): ('U', 'D'),\n",
    "    (1, 2): ('U', 'D', 'R'),\n",
    "    (2, 0): ('U', 'R'),\n",
    "    (2, 1): ('L', 'R'),\n",
    "    (2, 2): ('L', 'R', 'U'),\n",
    "    (2, 3): ('L', 'U'),\n",
    "    }\n",
    "    g.set(rewards, actions)\n",
    "    return g\n",
    "\n",
    "# Same as above but has a step cost / penalty for moving\n",
    "def negative_grid(step_cost=-0.1):\n",
    "    g = standard_grid()\n",
    "    g.rewards.update({\n",
    "    (0, 0): step_cost,\n",
    "    (0, 1): step_cost,\n",
    "    (0, 2): step_cost,\n",
    "    (1, 0): step_cost,\n",
    "    (1, 2): step_cost,\n",
    "    (2, 0): step_cost,\n",
    "    (2, 1): step_cost,\n",
    "    (2, 2): step_cost,\n",
    "    (2, 3): step_cost,\n",
    "    })\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0),\n",
       " (0, 1),\n",
       " (0, 2),\n",
       " (0, 3),\n",
       " (1, 0),\n",
       " (1, 2),\n",
       " (1, 3),\n",
       " (2, 0),\n",
       " (2, 1),\n",
       " (2, 2),\n",
       " (2, 3)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test out the grid\n",
    "g = Grid(width = 3, height = 4, start = (2,0))\n",
    "grid = standard_grid()\n",
    "grid.all_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prints a representation of the map with the values given \n",
    "def print_values(V, g):\n",
    "    for i in range(g.width):\n",
    "        print(\"---------------------------\")\n",
    "        for j in range(g.height):\n",
    "            v = V.get((i,j), 0)\n",
    "            if v >= 0:\n",
    "                print(\" %.2f|\" % v, end=\"\")\n",
    "            else:\n",
    "                print(\"%.2f|\" % v, end=\"\") # -ve sign takes up an extra space\n",
    "        print(\"\")\n",
    "\n",
    "# Prints out map with policy actions given each state\n",
    "def print_policy(P, g):\n",
    "    for i in range(g.width):\n",
    "        print(\"---------------------------\")\n",
    "        for j in range(g.height):\n",
    "            a = P.get((i,j), ' ')\n",
    "            print(\"  %s  |\" % a, end=\"\")\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative Policy Evaluation\n",
    "Given a policy, let's find it's value function V(s). We will do this for both a uniform random policy and fixed policy\n",
    "\n",
    "NOTE:\n",
    "There are 2 sources of randomness:\n",
    "\n",
    "1) p(a|s) - deciding what action to take given the state\n",
    "\n",
    "2) p(s',r|s,a) - the next state and reward given your action-state pair\n",
    "\n",
    "We are only modeling p(a|s) = uniform. How would the code change if p(s',r|s,a) is not deterministic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values for uniformly random actions:\n",
      "---------------------------\n",
      "-0.03| 0.09| 0.22| 0.00|\n",
      "---------------------------\n",
      "-0.16| 0.00|-0.44| 0.00|\n",
      "---------------------------\n",
      "-0.29|-0.41|-0.54|-0.77|\n",
      "\n",
      "\n",
      "\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  R  |     |\n",
      "---------------------------\n",
      "  U  |  R  |  R  |  U  |\n",
      "Values for fixed policy:\n",
      "---------------------------\n",
      " 0.81| 0.90| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.73| 0.00|-1.00| 0.00|\n",
      "---------------------------\n",
      " 0.66|-0.81|-0.90|-1.00|\n"
     ]
    }
   ],
   "source": [
    "SMALL_ENOUGH = 1e-3 # threshold for convergence\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    grid = standard_grid()\n",
    "    # States will be positions (i,j)\n",
    "    states = grid.all_states()\n",
    "\n",
    "    ### Uniformly random actions ###\n",
    "    # Initialize the value of each state to 0\n",
    "    V = {}\n",
    "    for s in states:\n",
    "        V[s] = 0\n",
    "    \n",
    "    # Repeat until convergence\n",
    "    gamma = 1.0 # discount factor\n",
    "    while True:\n",
    "        biggest_change = 0\n",
    "        for s in states:\n",
    "            old_v = V[s]\n",
    "            # V(s) only has value if it's not a terminal state\n",
    "            if s in grid.actions:\n",
    "                new_v = 0 # we will accumulate the answer\n",
    "                p_a = 1.0 / len(grid.actions[s]) # each action has equal probability\n",
    "                for a in grid.actions[s]:\n",
    "                    grid.set_state(s)\n",
    "                    r = grid.move(a)\n",
    "                    new_v += p_a * (r + gamma * V[grid.current_state()])\n",
    "                V[s] = new_v\n",
    "                biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
    "        if biggest_change < SMALL_ENOUGH:\n",
    "            break\n",
    "            \n",
    "    print(\"Values for uniformly random actions:\")\n",
    "    print_values(V, grid)\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    ### Fixed policy ###\n",
    "    policy = {\n",
    "    (2, 0): 'U',\n",
    "    (1, 0): 'U',\n",
    "    (0, 0): 'R',\n",
    "    (0, 1): 'R',\n",
    "    (0, 2): 'R',\n",
    "    (1, 2): 'R',\n",
    "    (2, 1): 'R',\n",
    "    (2, 2): 'R',\n",
    "    (2, 3): 'U',\n",
    "    }\n",
    "    print_policy(policy, grid)\n",
    "\n",
    "    V = {}\n",
    "    for s in states:\n",
    "        V[s] = 0\n",
    "\n",
    "    # Repeat until convergence\n",
    "    # Let's see how V(s) changes as we get further away from the reward\n",
    "    gamma = 0.9 # discount factor\n",
    "    while True:\n",
    "        biggest_change = 0\n",
    "        for s in states:\n",
    "            old_v = V[s]\n",
    "            # V(s) only has value if it's not a terminal state\n",
    "            if s in policy:\n",
    "                a = policy[s]\n",
    "                grid.set_state(s)\n",
    "                r = grid.move(a)\n",
    "                V[s] = r + gamma * V[grid.current_state()]\n",
    "                biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
    "\n",
    "        if biggest_change < SMALL_ENOUGH:\n",
    "            break\n",
    "    print(\"Values for fixed policy:\")\n",
    "    print_values(V, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Sources:\n",
    "    \n",
    "https://deeplearningcourses.com/c/artificial-intelligence-reinforcement-learning-in-python\n",
    "    \n",
    "https://www.udemy.com/artificial-intelligence-reinforcement-learning-in-python"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
