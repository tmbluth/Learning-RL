{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "When learning games, humans need experience. To become an expert we need lots of experience. First we must learn what the game's environment looks like, and how we can interact with said environment. Most importantly though, we learn how to win by studying past wins and failures.\n",
    "\n",
    "Computers fortunately can learn in a similar fashion. The environment and agent relationship must be defined, which is generally straightforward, and often tedious. The magic happens when we are able to calculate a value function and update each state's value. This is demonstrated with a simple reinforcement learning algorithm that can be applied for learning tic-tac-toe.\n",
    "\n",
    "Just use the update rule where V(s) is the value of the previous state, V(s') is the value of the next state, and alpha is the learning rate: \n",
    "\n",
    "V(s) = V(s) + alpha * (V(s') - V(s))\n",
    "\n",
    "Alpha is the hyperparameter that allows us to control how important new info is to updating our value function. If alpha is closer to 1 in this case then the value of the new state will override past info more easily. If its closer to 0 then the update of the new state is less influential. This can be good for when the agent is exploring new options in its environment, which brings us to the second hyperparameter we have control over in this situation, epsilon.\n",
    "\n",
    "In laymans terms, epsilon is the rate at which our agent explores its environment. Explore is this case means it will not choose the option that it has learned is most beneficial, but will explore a new action to see if that action is better than what it had previously learned. \n",
    "\n",
    "So how do the two work together? Well if you have a high learning rate and high epsilon, then your agent will explore often and use those new random explorations' values to \"override\" previous states' values, which would make the agent jump to conclusions over and over. We would rather have an agent use more careful learning methods and explore less, so having a lower alpha and epsilon would be smart (under 0.5). \n",
    "\n",
    "But, why take my word for it? Let's try out a few different hyperparameter combonations.\n",
    "\n",
    "Interesting things to try:\n",
    "1. What if they have different learning rates?\n",
    "2. What if they have different epsilons? (probability of exploring)\n",
    "3. What if the agent doesn't have enough experience?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Note: you may need to update your version of future: sudo pip install -U future\n",
    "from __future__ import print_function, division\n",
    "from builtins import range, input\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "LENGTH = 3\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, eps=0.1, alpha=0.5):\n",
    "        self.eps = eps # probability of choosing random action \n",
    "        self.alpha = alpha # learning rate\n",
    "        self.verbose = False\n",
    "        self.state_history = []\n",
    "\n",
    "    def setV(self, V):\n",
    "        self.V = V\n",
    "    \n",
    "    # Assigns symbol to Agent, whether that be 1 (O) or -1 (X)\n",
    "    def set_symbol(self, sym):\n",
    "        self.sym = sym\n",
    "        \n",
    "    # If true, will print values for each position on the board\n",
    "    def set_verbose(self, v):\n",
    "        self.verbose = v\n",
    "    \n",
    "    # Erases all historical states into empty list\n",
    "    def reset_history(self):\n",
    "        self.state_history = []\n",
    "        \n",
    "    # Choose an action based on epsilon-greedy strategy\n",
    "    def take_action(self, env):\n",
    "        r = np.random.rand()\n",
    "        best_state = None\n",
    "        # If random number from 0 to 1 is less than our epsilon value then take a random action\n",
    "        if r < self.eps:\n",
    "            if self.verbose:\n",
    "                print(\"Taking a random action\")\n",
    "            # Look at all possible moves by remembering which of the positions are open...\n",
    "            possible_moves = []\n",
    "            for i in range(LENGTH):\n",
    "                for j in range(LENGTH):\n",
    "                    if env.is_empty(i, j):\n",
    "                        possible_moves.append((i, j))\n",
    "            # ...and randomly choosing one of those positions\n",
    "            idx = np.random.choice(len(possible_moves))\n",
    "            next_move = possible_moves[idx]\n",
    "        else:\n",
    "            # Choose the best action based on current values of states\n",
    "            # Loop through all possible moves, get their values and keep track of the best value\n",
    "            pos2value = {} # for debugging\n",
    "            next_move = None\n",
    "            best_value = -1\n",
    "            for i in range(LENGTH):\n",
    "                for j in range(LENGTH):\n",
    "                    if env.is_empty(i, j):\n",
    "                    # what is the state if we made this move?\n",
    "                        env.board[i,j] = self.sym\n",
    "                        state = env.get_state()\n",
    "                        env.board[i,j] = 0 # don't forget to change it back!\n",
    "                        pos2value[(i,j)] = self.V[state]\n",
    "                        if self.V[state] > best_value:\n",
    "                            best_value = self.V[state]\n",
    "                            best_state = state\n",
    "                            next_move = (i, j)\n",
    "\n",
    "            # if verbose, draw the board w/ the values\n",
    "            if self.verbose:\n",
    "                print(\"Taking a greedy action\")\n",
    "                for i in range(LENGTH):\n",
    "                    print(\"------------------\")\n",
    "                    for j in range(LENGTH):\n",
    "                        if env.is_empty(i, j):\n",
    "                            # print the value\n",
    "                            print(\" %.2f|\" % pos2value[(i,j)], end=\"\")\n",
    "                        else:\n",
    "                            print(\"  \", end=\"\")\n",
    "                            if env.board[i,j] == env.x:\n",
    "                                print(\"x  |\", end=\"\")\n",
    "                            elif env.board[i,j] == env.o:\n",
    "                                print(\"o  |\", end=\"\")\n",
    "                            else:\n",
    "                                print(\"   |\", end=\"\")\n",
    "                    print(\"\")\n",
    "                print(\"------------------\")\n",
    "\n",
    "        # make the move\n",
    "        env.board[next_move[0], next_move[1]] = self.sym\n",
    "\n",
    "    def update_state_history(self, s):\n",
    "        # Not in take_action, because take_action happens for each player\n",
    "        # State history needs to be updated every iteration\n",
    "        # s = env.get_state() # don't want to do this twice so pass it in\n",
    "        self.state_history.append(s)\n",
    "\n",
    "    def update(self, env):\n",
    "        # We want to BACKTRACK over the states, so that:\n",
    "        # V(prev_state) = V(prev_state) + alpha*(V(next_state) - V(prev_state)) where V(next_state) = reward if it's the most current state\n",
    "        # NOTE: we ONLY do this at the end of an episode\n",
    "        reward = env.reward(self.sym)\n",
    "        target = reward\n",
    "        for prev in reversed(self.state_history):\n",
    "            value = self.V[prev] + self.alpha*(target - self.V[prev])\n",
    "            self.V[prev] = value\n",
    "            target = value\n",
    "        self.reset_history()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This class represents a tic-tac-toe game environment\n",
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.board = np.zeros((LENGTH, LENGTH))\n",
    "        self.x = -1 # represents an x on the board, player 1\n",
    "        self.o = 1 # represents an o on the board, player 2\n",
    "        self.winner = None\n",
    "        self.ended = False\n",
    "        self.num_states = 3**(LENGTH*LENGTH)\n",
    "\n",
    "    def is_empty(self, i, j):\n",
    "        return self.board[i,j] == 0\n",
    "\n",
    "    def reward(self, sym):\n",
    "        # No reward until game is over\n",
    "        if not self.game_over():\n",
    "            return 0\n",
    "        # If we get here, game is over\n",
    "        # sym will be self.x or self.o\n",
    "        return 1 if self.winner == sym else 0\n",
    "\n",
    "    def get_state(self):\n",
    "        # Returns the current state, represented as an int from 0...|S|-1, where S = set of all possible states\n",
    "        # |S| = 3^(BOARD SIZE), since each cell can have 3 possible values - empty, x, o\n",
    "        # Some states are not possible, e.g. all cells are x, but we ignore that detail\n",
    "        k = 0\n",
    "        h = 0\n",
    "        for i in range(LENGTH):\n",
    "            for j in range(LENGTH):\n",
    "                if self.board[i,j] == 0:\n",
    "                    v = 0\n",
    "                elif self.board[i,j] == self.x:\n",
    "                    v = 1\n",
    "                elif self.board[i,j] == self.o:\n",
    "                    v = 2\n",
    "                h += (3**k) * v\n",
    "                k += 1\n",
    "        return h\n",
    "\n",
    "    def game_over(self, force_recalculate=False):\n",
    "        # Returns true if game over (a player has won or it's a draw) otherwise returns false\n",
    "        # Also sets 'winner' instance variable and 'ended' instance variable\n",
    "        if not force_recalculate and self.ended:\n",
    "            return self.ended\n",
    "\n",
    "        # Check rows\n",
    "        for i in range(LENGTH):\n",
    "            for player in (self.x, self.o):\n",
    "                if self.board[i].sum() == player*LENGTH:\n",
    "                    self.winner = player\n",
    "                    self.ended = True\n",
    "                    return True\n",
    "\n",
    "        # Check columns\n",
    "        for j in range(LENGTH):\n",
    "            for player in (self.x, self.o):\n",
    "                if self.board[:,j].sum() == player*LENGTH:\n",
    "                    self.winner = player\n",
    "                    self.ended = True\n",
    "                    return True\n",
    "\n",
    "        # Check diagonals\n",
    "        for player in (self.x, self.o):\n",
    "            # top-left -> bottom-right diagonal\n",
    "            if self.board.trace() == player*LENGTH:\n",
    "                self.winner = player\n",
    "                self.ended = True\n",
    "                return True\n",
    "            # top-right -> bottom-left diagonal\n",
    "            if np.fliplr(self.board).trace() == player*LENGTH:\n",
    "                self.winner = player\n",
    "                self.ended = True\n",
    "                return True\n",
    "\n",
    "        # Check if draw\n",
    "        if np.all((self.board == 0) == False):\n",
    "            # winner stays None\n",
    "            self.winner = None\n",
    "            self.ended = True\n",
    "            return True\n",
    "\n",
    "        # Game is not over\n",
    "        self.winner = None\n",
    "        return False\n",
    "\n",
    "    def is_draw(self):\n",
    "        return self.ended and self.winner is None\n",
    "\n",
    "    # Example board\n",
    "    # -------------\n",
    "    # | x |   |   |\n",
    "    # -------------\n",
    "    # |   |   |   |\n",
    "    # -------------\n",
    "    # |   |   | o |\n",
    "    # -------------\n",
    "    def draw_board(self):\n",
    "        for i in range(LENGTH):\n",
    "            print(\"-------------\")\n",
    "            for j in range(LENGTH):\n",
    "                print(\"  \", end=\"\")\n",
    "                if self.board[i,j] == self.x:\n",
    "                    print(\"x \", end=\"\")\n",
    "                elif self.board[i,j] == self.o:\n",
    "                    print(\"o \", end=\"\")\n",
    "                else:\n",
    "                    print(\"  \", end=\"\")\n",
    "            print(\"\")\n",
    "        print(\"-------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Human:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def set_symbol(self, sym):\n",
    "        self.sym = sym\n",
    "\n",
    "    def take_action(self, env):\n",
    "        while True:\n",
    "        # break if we make a legal move\n",
    "            move = input(\"Enter coordinates i,j for your next move (i,j=0..2): \")\n",
    "            i, j = move.split(',')\n",
    "            i = int(i)\n",
    "            j = int(j)\n",
    "            if env.is_empty(i, j):\n",
    "                env.board[i,j] = self.sym\n",
    "                break\n",
    "\n",
    "    def update(self, env):\n",
    "        pass\n",
    "\n",
    "    def update_state_history(self, s):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Recursive function that will return all possible states (as ints) and who the corresponding winner is for those states (if any)\n",
    "# (i, j) refers to the next cell on the board to permute (we need to try -1, 0, 1)\n",
    "# Impossible games are ignored, i.e. 3x's and 3o's in a row simultaneously since that will never happen in a real game\n",
    "def get_state_hash_and_winner(env, i=0, j=0):\n",
    "    results = []\n",
    "    for v in (0, env.x, env.o):\n",
    "        env.board[i,j] = v # if empty board it should already be 0\n",
    "        if j == 2:\n",
    "            # j goes back to 0, increase i, unless i = 2, then we are done\n",
    "            if i == 2:\n",
    "                # The board is full, collect results and return\n",
    "                state = env.get_state()\n",
    "                ended = env.game_over(force_recalculate=True)\n",
    "                winner = env.winner\n",
    "                results.append((state, winner, ended))\n",
    "            else:\n",
    "                results += get_state_hash_and_winner(env, i + 1, 0)\n",
    "        else:\n",
    "        # Increment j, i stays the same\n",
    "            results += get_state_hash_and_winner(env, i, j + 1)\n",
    "\n",
    "    return results\n",
    "\n",
    "def initialV_x(env, state_winner_triples):\n",
    "    # Initialize state values as follows:\n",
    "    # If x wins, V(s) = 1\n",
    "    # If x loses or draw, V(s) = 0\n",
    "    # Otherwise, V(s) = 0.5\n",
    "    V = np.zeros(env.num_states)\n",
    "    for state, winner, ended in state_winner_triples:\n",
    "        if ended:\n",
    "            if winner == env.x:\n",
    "                v = 1\n",
    "            else:\n",
    "                v = 0\n",
    "        else:\n",
    "            v = 0.5\n",
    "        V[state] = v\n",
    "    return V\n",
    "\n",
    "\n",
    "def initialV_o(env, state_winner_triples):\n",
    "    # This is (almost) the opposite of initial V for player x since everywhere where x wins (1), o loses (0)\n",
    "    V = np.zeros(env.num_states)\n",
    "    for state, winner, ended in state_winner_triples:\n",
    "        if ended:\n",
    "            if winner == env.o:\n",
    "                v = 1\n",
    "            else:\n",
    "                v = 0\n",
    "        else:\n",
    "            v = 0.5\n",
    "        V[state] = v\n",
    "    return V\n",
    "\n",
    "\n",
    "def play_game(p1, p2, env, draw=False):\n",
    "    # Loops until the game is over\n",
    "    current_player = None\n",
    "    while not env.game_over():\n",
    "        # Alternate between players\n",
    "        # p1 always starts first\n",
    "        if current_player == p1:\n",
    "            current_player = p2\n",
    "        else:\n",
    "            current_player = p1\n",
    "\n",
    "        # Draw the board before the user who wants to see it makes a move\n",
    "        if draw:\n",
    "            if draw == 1 and current_player == p1:\n",
    "                env.draw_board()\n",
    "            if draw == 2 and current_player == p2:\n",
    "                env.draw_board()\n",
    "\n",
    "        # Current player makes a move\n",
    "        current_player.take_action(env)\n",
    "\n",
    "        # Update state histories\n",
    "        state = env.get_state()\n",
    "        p1.update_state_history(state)\n",
    "        p2.update_state_history(state)\n",
    "\n",
    "    if draw:\n",
    "        env.draw_board()\n",
    "\n",
    "    # Do the value function update\n",
    "    p1.update(env)\n",
    "    p2.update(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train p2\n",
      "0\n",
      "200\n",
      "400\n",
      "600\n",
      "800\n",
      "Train p3\n",
      "0\n",
      "Train p4\n",
      "0\n",
      "200\n",
      "400\n",
      "600\n",
      "800\n",
      "Train p5\n",
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "Train p6\n",
      "0\n",
      "200\n",
      "400\n",
      "600\n",
      "800\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Train the agents\n",
    "    p1 = Agent(eps = 0.1, alpha = 0.5)  # This is the baseline\n",
    "    p2 = Agent(eps = 0.9, alpha = 0.5)  # High epsilson\n",
    "    p3 = Agent(eps = 0.1, alpha = 0.01) # Low alpha and few training rounds\n",
    "    p4 = Agent(eps = 0.1, alpha = 0.01) # Low alpha and many training rounds\n",
    "    p5 = Agent(eps = 0.1, alpha = 0.9)  # High alpha and few training rounds\n",
    "    p6 = Agent(eps = 0.1, alpha = 0.9)  # High alpha and many training rounds\n",
    "\n",
    "    # Set initial V for p1 and p2\n",
    "    env = Environment()\n",
    "    state_winner_triples = get_state_hash_and_winner(env)\n",
    "\n",
    "    Vx = initialV_x(env, state_winner_triples)\n",
    "    Vo = initialV_o(env, state_winner_triples)\n",
    "    \n",
    "    p1.setV(Vo) # p1 is always 'o'\n",
    "    p2.setV(Vx) \n",
    "    p3.setV(Vx)\n",
    "    p4.setV(Vx)\n",
    "    p5.setV(Vx)\n",
    "    p6.setV(Vx)\n",
    "\n",
    "    # Give each player their symbol\n",
    "    p1.set_symbol(env.o) # p1 is always 'o'\n",
    "    p2.set_symbol(env.x)\n",
    "    p3.set_symbol(env.x)\n",
    "    p4.set_symbol(env.x)\n",
    "    p5.set_symbol(env.x)\n",
    "    p6.set_symbol(env.x)\n",
    "    \n",
    "    high_exp = 1000\n",
    "    low_exp = 10\n",
    "    \n",
    "    print(\"Train p2\")    \n",
    "    for t in range(high_exp):\n",
    "        if t % 200 == 0:\n",
    "            print(t)\n",
    "        play_game(p2, p1, Environment())\n",
    "        \n",
    "    print(\"Train p3\")        \n",
    "    for t in range(low_exp):\n",
    "        if t % 20 == 0:\n",
    "            print(t)\n",
    "        play_game(p3, p1, Environment())  \n",
    "        \n",
    "    print(\"Train p4\")\n",
    "    for t in range(high_exp):\n",
    "        if t % 200 == 0:\n",
    "            print(t)\n",
    "        play_game(p4, p1, Environment())\n",
    "        \n",
    "    print(\"Train p5\")    \n",
    "    for t in range(low_exp):\n",
    "        if t % 2 == 0:\n",
    "            print(t)\n",
    "        play_game(p5, p1, Environment())  \n",
    "        \n",
    "    print(\"Train p6\")    \n",
    "    for t in range(high_exp):\n",
    "        if t % 200 == 0:\n",
    "            print(t)\n",
    "        play_game(p6, p1, Environment()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I'm just starting off I want to give myself a chance against this first agent. p1 has been trained as a baseline opponent against all the other agents, so it has plenty of experience. Do note though, p1 has consistently been trained as the second player, so it knows how to act in response to another player starting the game. Let's see if I can beat it given I have the upper hand in going first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------\n",
      "            \n",
      "-------------\n",
      "            \n",
      "-------------\n",
      "            \n",
      "-------------\n",
      "Enter coordinates i,j for your next move (i,j=0..2): 1,1\n",
      "Taking a greedy action\n",
      "------------------\n",
      " 0.00| 0.00| 0.00|\n",
      "------------------\n",
      " 0.00|  x  | 0.00|\n",
      "------------------\n",
      " 0.00| 0.18| 0.00|\n",
      "------------------\n",
      "-------------\n",
      "            \n",
      "-------------\n",
      "      x     \n",
      "-------------\n",
      "      o     \n",
      "-------------\n",
      "Enter coordinates i,j for your next move (i,j=0..2): 0,2\n",
      "Taking a greedy action\n",
      "------------------\n",
      " 0.00| 0.00|  x  |\n",
      "------------------\n",
      " 0.00|  x  | 0.00|\n",
      "------------------\n",
      " 0.25|  o  | 0.00|\n",
      "------------------\n",
      "-------------\n",
      "          x \n",
      "-------------\n",
      "      x     \n",
      "-------------\n",
      "  o   o     \n",
      "-------------\n",
      "Enter coordinates i,j for your next move (i,j=0..2): 2,2\n",
      "Taking a greedy action\n",
      "------------------\n",
      " 0.00| 0.00|  x  |\n",
      "------------------\n",
      " 0.00|  x  | 0.00|\n",
      "------------------\n",
      "  o  |  o  |  x  |\n",
      "------------------\n",
      "-------------\n",
      "          x \n",
      "-------------\n",
      "      x   o \n",
      "-------------\n",
      "  o   o   x \n",
      "-------------\n",
      "Enter coordinates i,j for your next move (i,j=0..2): 0,0\n",
      "-------------\n",
      "  x       x \n",
      "-------------\n",
      "      x   o \n",
      "-------------\n",
      "  o   o   x \n",
      "-------------\n",
      "Play again? [Y/n]: n\n"
     ]
    }
   ],
   "source": [
    "human = Human()\n",
    "human.set_symbol(env.x) # Only this time my symbol is 'x' because p1 was 'o' while training the other agents.\n",
    "\n",
    "# Human vs. player 1 (eps = 0.1,  alpha = 0.5). Human goes first\n",
    "while True:\n",
    "    p1.set_verbose(True)\n",
    "    play_game(human, p1, Environment(), draw=1)\n",
    "    answer = input(\"Play again? [Y/n]: \")\n",
    "    if answer and answer.lower()[0] == 'n':\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phew, I did it. It knew to block me but since I started I used a strategy to outsmart it. Let's see how well I can do with the other agents. These times they will be starting the game (since draw = 2 in play game). The good thing for me is that p2 will randomly explore 90% of the time, so even with it starting our game, I am at a huge advantage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taking a random action\n",
      "-------------\n",
      "            \n",
      "-------------\n",
      "          x \n",
      "-------------\n",
      "            \n",
      "-------------\n",
      "Enter coordinates i,j for your next move (i,j=0..2): 1,1\n",
      "Taking a random action\n",
      "-------------\n",
      "  x         \n",
      "-------------\n",
      "      o   x \n",
      "-------------\n",
      "            \n",
      "-------------\n",
      "Enter coordinates i,j for your next move (i,j=0..2): 2,1\n",
      "Taking a greedy action\n",
      "------------------\n",
      "  x  | 0.25| 0.00|\n",
      "------------------\n",
      " 0.20|  o  |  x  |\n",
      "------------------\n",
      " 0.00|  o  | 0.00|\n",
      "------------------\n",
      "-------------\n",
      "  x   x     \n",
      "-------------\n",
      "      o   x \n",
      "-------------\n",
      "      o     \n",
      "-------------\n",
      "Enter coordinates i,j for your next move (i,j=0..2): 0,2\n",
      "Taking a random action\n",
      "-------------\n",
      "  x   x   o \n",
      "-------------\n",
      "  x   o   x \n",
      "-------------\n",
      "      o     \n",
      "-------------\n",
      "Enter coordinates i,j for your next move (i,j=0..2): 2,0\n",
      "-------------\n",
      "  x   x   o \n",
      "-------------\n",
      "  x   o   x \n",
      "-------------\n",
      "  o   o     \n",
      "-------------\n",
      "Play again? [Y/n]: n\n"
     ]
    }
   ],
   "source": [
    "human = Human()\n",
    "human.set_symbol(env.o) # I will now be 'o' since they were trained as 'x'\n",
    "\n",
    "# Human vs. player 2 (eps = 0.9,  alpha = 0.5). Player 2 goes first\n",
    "while True:\n",
    "    p2.set_verbose(True)\n",
    "    play_game(p2, human, Environment(), draw=2)\n",
    "    answer = input(\"Play again? [Y/n]: \")\n",
    "    if answer and answer.lower()[0] == 'n':\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately for me p2 had a 90% chance of making a random decision which hurt its decision making. I was quickly able to get the best of it since it did not exploit it's stragegy, but was looking for another solution. Now for something a bit more challenging. p3 has a low epsilon, which means it will explore less and exploit more. It also will only update incrementally so that it will slowly converge on an optimal strategy. This one however, will only have played 10 games. Let's see if it learned enough in that time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taking a greedy action\n",
      "------------------\n",
      " 0.12| 0.10| 0.32|\n",
      "------------------\n",
      " 0.17| 0.99| 0.03|\n",
      "------------------\n",
      " 0.11| 0.26| 0.29|\n",
      "------------------\n",
      "-------------\n",
      "            \n",
      "-------------\n",
      "      x     \n",
      "-------------\n",
      "            \n",
      "-------------\n",
      "Enter coordinates i,j for your next move (i,j=0..2): 2,2\n",
      "Taking a greedy action\n",
      "------------------\n",
      " 0.50| 0.54| 0.19|\n",
      "------------------\n",
      " 0.91|  x  | 0.93|\n",
      "------------------\n",
      " 0.76| 0.76|  o  |\n",
      "------------------\n",
      "-------------\n",
      "            \n",
      "-------------\n",
      "      x   x \n",
      "-------------\n",
      "          o \n",
      "-------------\n",
      "Enter coordinates i,j for your next move (i,j=0..2): 1,0\n",
      "Taking a greedy action\n",
      "------------------\n",
      " 0.17| 0.50| 0.14|\n",
      "------------------\n",
      "  o  |  x  |  x  |\n",
      "------------------\n",
      " 0.50| 0.95|  o  |\n",
      "------------------\n",
      "-------------\n",
      "            \n",
      "-------------\n",
      "  o   x   x \n",
      "-------------\n",
      "      x   o \n",
      "-------------\n",
      "Enter coordinates i,j for your next move (i,j=0..2): 0,1\n",
      "Taking a greedy action\n",
      "------------------\n",
      " 0.04|  o  | 0.14|\n",
      "------------------\n",
      "  o  |  x  |  x  |\n",
      "------------------\n",
      " 0.50|  x  |  o  |\n",
      "------------------\n",
      "-------------\n",
      "      o     \n",
      "-------------\n",
      "  o   x   x \n",
      "-------------\n",
      "  x   x   o \n",
      "-------------\n",
      "Enter coordinates i,j for your next move (i,j=0..2): 0,2\n",
      "Taking a greedy action\n",
      "------------------\n",
      " 0.00|  o  |  o  |\n",
      "------------------\n",
      "  o  |  x  |  x  |\n",
      "------------------\n",
      "  x  |  x  |  o  |\n",
      "------------------\n",
      "-------------\n",
      "  x   o   o \n",
      "-------------\n",
      "  o   x   x \n",
      "-------------\n",
      "  x   x   o \n",
      "-------------\n",
      "Play again? [Y/n]: n\n"
     ]
    }
   ],
   "source": [
    "human = Human()\n",
    "human.set_symbol(env.o)\n",
    "\n",
    "# Human vs. player 3 (eps = 0.1,  alpha = 0.01) with 10 training rounds. Player 3 goes first\n",
    "while True:\n",
    "    p3.set_verbose(True)\n",
    "    play_game(p3, human, Environment(), draw=2)\n",
    "    answer = input(\"Play again? [Y/n]: \")\n",
    "    if answer and answer.lower()[0] == 'n':\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, now we are getting somewhere. With only 10 rounds it actually learned some decent strategy. The very low epsilon gives such a low chance for exploration that all the actions taken were \"greedy\" or in other words, the maximized the chance of reward. This is also known as exploitation.\n",
    "\n",
    "Now I'll be doing the same looking agent but with many more rounds of training to see if more experience pays off in this scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taking a greedy action\n",
      "------------------\n",
      " 0.33| 0.04| 0.05|\n",
      "------------------\n",
      " 0.03| 0.68| 0.18|\n",
      "------------------\n",
      " 0.30| 0.79| 0.04|\n",
      "------------------\n",
      "-------------\n",
      "            \n",
      "-------------\n",
      "            \n",
      "-------------\n",
      "      x     \n",
      "-------------\n",
      "Enter coordinates i,j for your next move (i,j=0..2): 1,1\n",
      "Taking a greedy action\n",
      "------------------\n",
      " 0.03| 0.04| 0.01|\n",
      "------------------\n",
      " 0.01|  o  | 0.03|\n",
      "------------------\n",
      " 0.02|  x  | 0.03|\n",
      "------------------\n",
      "-------------\n",
      "      x     \n",
      "-------------\n",
      "      o     \n",
      "-------------\n",
      "      x     \n",
      "-------------\n",
      "Enter coordinates i,j for your next move (i,j=0..2): 0,0\n",
      "Taking a greedy action\n",
      "------------------\n",
      "  o  |  x  | 0.01|\n",
      "------------------\n",
      " 0.01|  o  | 0.06|\n",
      "------------------\n",
      " 0.50|  x  | 0.13|\n",
      "------------------\n",
      "-------------\n",
      "  o   x     \n",
      "-------------\n",
      "      o     \n",
      "-------------\n",
      "  x   x     \n",
      "-------------\n",
      "Enter coordinates i,j for your next move (i,j=0..2): 2,2\n",
      "-------------\n",
      "  o   x     \n",
      "-------------\n",
      "      o     \n",
      "-------------\n",
      "  x   x   o \n",
      "-------------\n",
      "Play again? [Y/n]: n\n"
     ]
    }
   ],
   "source": [
    "human = Human()\n",
    "human.set_symbol(env.o)\n",
    "\n",
    "# Human vs. player 4 (eps = 0.1,  alpha = 0.01) with 1000 rounds of training. Player 4 goes first\n",
    "while True:\n",
    "    p4.set_verbose(True)\n",
    "    play_game(p4, human, Environment(), draw=2)\n",
    "    answer = input(\"Play again? [Y/n]: \")\n",
    "    if answer and answer.lower()[0] == 'n':\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How strange! With more training this agent performed much worse. I am honstly shocked at how poorly it performed given how much it trained. It must have memorized sequences of moves, but not learned general stragety. Perhaps it fell into a local optimum set of decisions based off who it was trained against (p1), but when pitted against a human, did not know what to do. \n",
    "\n",
    "Now let's see how a high learning rate agent does with only 10 rounds of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taking a greedy action\n",
      "------------------\n",
      " 0.33| 0.04| 0.05|\n",
      "------------------\n",
      " 0.03| 0.68| 0.18|\n",
      "------------------\n",
      " 0.30| 0.79| 0.04|\n",
      "------------------\n",
      "-------------\n",
      "            \n",
      "-------------\n",
      "            \n",
      "-------------\n",
      "      x     \n",
      "-------------\n",
      "Enter coordinates i,j for your next move (i,j=0..2): 1,1\n",
      "Taking a greedy action\n",
      "------------------\n",
      " 0.03| 0.04| 0.01|\n",
      "------------------\n",
      " 0.01|  o  | 0.03|\n",
      "------------------\n",
      " 0.02|  x  | 0.03|\n",
      "------------------\n",
      "-------------\n",
      "      x     \n",
      "-------------\n",
      "      o     \n",
      "-------------\n",
      "      x     \n",
      "-------------\n",
      "Enter coordinates i,j for your next move (i,j=0..2): 0,2\n",
      "Taking a greedy action\n",
      "------------------\n",
      " 0.01|  x  |  o  |\n",
      "------------------\n",
      " 0.01|  o  | 0.12|\n",
      "------------------\n",
      " 0.17|  x  | 0.03|\n",
      "------------------\n",
      "-------------\n",
      "      x   o \n",
      "-------------\n",
      "      o     \n",
      "-------------\n",
      "  x   x     \n",
      "-------------\n",
      "Enter coordinates i,j for your next move (i,j=0..2): 2,2\n",
      "Taking a greedy action\n",
      "------------------\n",
      " 0.05|  x  |  o  |\n",
      "------------------\n",
      " 0.50|  o  | 0.05|\n",
      "------------------\n",
      "  x  |  x  |  o  |\n",
      "------------------\n",
      "-------------\n",
      "      x   o \n",
      "-------------\n",
      "  x   o     \n",
      "-------------\n",
      "  x   x   o \n",
      "-------------\n",
      "Enter coordinates i,j for your next move (i,j=0..2): 1,2\n",
      "-------------\n",
      "      x   o \n",
      "-------------\n",
      "  x   o   o \n",
      "-------------\n",
      "  x   x   o \n",
      "-------------\n",
      "Play again? [Y/n]: n\n"
     ]
    }
   ],
   "source": [
    "human = Human()\n",
    "human.set_symbol(env.o)\n",
    "\n",
    "# Human vs. player 5 (eps = 0.1,  alpha = 0.9) with 10 rounds of training. Player 5 goes first\n",
    "while True:\n",
    "    p5.set_verbose(True)\n",
    "    play_game(p5, human, Environment(), draw=2)\n",
    "    answer = input(\"Play again? [Y/n]: \")\n",
    "    if answer and answer.lower()[0] == 'n':\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p5 behaved similarly to the p4. Its as if they tried going for the win without considering my moves or trying to block me. This case makes more sense to me however, since its learning rate was so high. It was basically going off its last game more than its prior game(s) which made it very biased toward the final training game it went through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taking a greedy action\n",
      "------------------\n",
      " 0.33| 0.04| 0.05|\n",
      "------------------\n",
      " 0.03| 0.68| 0.18|\n",
      "------------------\n",
      " 0.30| 0.14| 0.04|\n",
      "------------------\n",
      "-------------\n",
      "            \n",
      "-------------\n",
      "      x     \n",
      "-------------\n",
      "            \n",
      "-------------\n",
      "Enter coordinates i,j for your next move (i,j=0..2): 0,0\n",
      "Taking a greedy action\n",
      "------------------\n",
      "  o  | 0.99| 0.13|\n",
      "------------------\n",
      " 0.17|  x  | 0.15|\n",
      "------------------\n",
      " 0.03| 0.08| 0.03|\n",
      "------------------\n",
      "-------------\n",
      "  o   x     \n",
      "-------------\n",
      "      x     \n",
      "-------------\n",
      "            \n",
      "-------------\n",
      "Enter coordinates i,j for your next move (i,j=0..2): 2,1\n",
      "Taking a random action\n",
      "-------------\n",
      "  o   x     \n",
      "-------------\n",
      "      x     \n",
      "-------------\n",
      "  x   o     \n",
      "-------------\n",
      "Enter coordinates i,j for your next move (i,j=0..2): 0,2\n",
      "Taking a greedy action\n",
      "------------------\n",
      "  o  |  x  |  o  |\n",
      "------------------\n",
      " 0.09|  x  | 0.06|\n",
      "------------------\n",
      "  x  |  o  | 0.01|\n",
      "------------------\n",
      "-------------\n",
      "  o   x   o \n",
      "-------------\n",
      "  x   x     \n",
      "-------------\n",
      "  x   o     \n",
      "-------------\n",
      "Enter coordinates i,j for your next move (i,j=0..2): 1,2\n",
      "Taking a greedy action\n",
      "------------------\n",
      "  o  |  x  |  o  |\n",
      "------------------\n",
      "  x  |  x  |  o  |\n",
      "------------------\n",
      "  x  |  o  | 0.00|\n",
      "------------------\n",
      "-------------\n",
      "  o   x   o \n",
      "-------------\n",
      "  x   x   o \n",
      "-------------\n",
      "  x   o   x \n",
      "-------------\n",
      "Play again? [Y/n]: n\n"
     ]
    }
   ],
   "source": [
    "human = Human()\n",
    "human.set_symbol(env.o)\n",
    "\n",
    "# Human vs. player 5 (eps = 0.1,  alpha = 0.9) with 1000 rounds of training. Player 5 goes first\n",
    "while True:\n",
    "    p6.set_verbose(True)\n",
    "    play_game(p6, human, Environment(), draw=2)\n",
    "    answer = input(\"Play again? [Y/n]: \")\n",
    "    if answer and answer.lower()[0] == 'n':\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p6 actually seemed quite smart. Though it was relying more on its later training games, it must have gone through so many that the learning rate was not as critical at some point. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary:\n",
    "\n",
    "There are a limited number of actions and states so tic-tac-toe may be too simple to really tweak the reinforcement learning hyperparameters to get super varied results, but this does give a view into the control one may have when training these agents. However, it looks as though agents/players have very dramatic performance differences when epsilon is shifted between 0 and 1. This makes sense, since epsilon is the cutoff that determines the \"chance\" of the agent choosing a random action to explore other states. When playing an actual player, any chance of random play could cost you the game.\n",
    "\n",
    "Hopefully this can help you get a feel for reinforcement learning on a simple game and will allow you to try to take the next step towards bigger games, and eventually into practical applications. Good luck learning!\n",
    "\n",
    "Sources:\n",
    "\n",
    "https://deeplearningcourses.com/c/artificial-intelligence-reinforcement-learning-in-python\n",
    "\n",
    "https://www.udemy.com/artificial-intelligence-reinforcement-learning-in-python"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
