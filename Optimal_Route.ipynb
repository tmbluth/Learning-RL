{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal Route\n",
    "\n",
    "How can you know the fastest way to get from point A to point B? Well you need to know the distance between points, of course. But what if there are multiple non-linear ways to get there? What then?\n",
    "\n",
    "This is where experience comes in. Since we have taken certain paths to get from one point to another we know which path is the best. This is only discovered once we:\n",
    "1) reach the end of the trip and\n",
    "2) have tried multiple trip paths to know the best one.\n",
    "\n",
    "Machines can learn the same way! We can provide an agent its environment, tell it where to start, where to end, and where to avoid. The agent will be rewarded only when it reaches its destination. Just like humans don't like waiting in traffic or taking all day to get somewhere, we can discount its final reward by diminishing its reward over time/different states. This will \"motivate\" the agent to pick routes that took as little time as possible. \n",
    "\n",
    "I'll set up a deterministic discrete 4 x 3 environment where there are states and actions that occur in turns. \n",
    "\n",
    "The agent will make an policy of moves it should take only once it reaches a terminal state, which in this case is 1 or -1. Once that maximum reward is found it will look back on which moves it took to get to that point. The values of each state will be the terminal state (either 1 or -1) + the value of the previous state + the value of the previous state, etc. Since the value of each state will be 0 the end result will just be 1, which is problematic if our agent is trying to maximize its reward. Why couldn't it just wander for a long period of time before transitioning to the +1 state and call THAT its optimal policy? This is where the discounting hyperparameter comes in.\n",
    "\n",
    "If we can discount that reward of 1 with -0.1 each time the agent makes a transition between states, then it will want to make as few moves as possible to to maximize its reward. Once it does get to the end, it looks back at the moves it took to get there and calculates the value. So if it took 10 moves, the reward would be 0. Then it tries again. If the second round only took 6 moves, then the reward is 0.4 which is greater. The process repeats until the gain in reward no longer improves. Let's try it out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note: you may need to update your version of future\n",
    "# sudo pip install -U future\n",
    "from __future__ import print_function, division\n",
    "from builtins import range\n",
    "import numpy as np\n",
    "\n",
    "class Grid: # Environment\n",
    "    def __init__(self, width, height, start):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.i = start[0]\n",
    "        self.j = start[1]\n",
    "    \n",
    "    # This function makes rewards and actions per state into class attributes but is added to the class once they are specified\n",
    "    def set(self, rewards, actions):\n",
    "        # rewards should be a dict of: (i, j): r (row, col): reward\n",
    "        # actions should be a dict of: (i, j): A (row, col): list of possible actions\n",
    "        self.rewards = rewards\n",
    "        self.actions = actions\n",
    "    \n",
    "    # Takes a tuple of a state and assigns each x and y index to i and j\n",
    "    def set_state(self, s):\n",
    "        self.i = s[0]\n",
    "        self.j = s[1]\n",
    "    \n",
    "    # Returns tuple of current state\n",
    "    def current_state(self):\n",
    "        return (self.i, self.j)\n",
    "    \n",
    "    # Checks if agent has reached a terminal state\n",
    "    def is_terminal(self, s):\n",
    "        return s not in self.actions\n",
    "    \n",
    "    # How the agent interacts with the environment\n",
    "    def move(self, action):\n",
    "        # Check if legal move first\n",
    "        if action in self.actions[(self.i, self.j)]:\n",
    "            if action == 'U':\n",
    "                self.i -= 1\n",
    "            elif action == 'D':\n",
    "                self.i += 1\n",
    "            elif action == 'R':\n",
    "                self.j += 1\n",
    "            elif action == 'L':\n",
    "                self.j -= 1\n",
    "        # Return a reward from that state if any. If none, return 0\n",
    "        return self.rewards.get((self.i, self.j), 0)\n",
    "    \n",
    "    # Used to backtrack as the optimal policy is calculated\n",
    "    def undo_move(self, action):\n",
    "        # These are the opposite of what U/D/L/R should normally do\n",
    "        if action == 'U':\n",
    "            self.i += 1\n",
    "        elif action == 'D':\n",
    "            self.i -= 1\n",
    "        elif action == 'R':\n",
    "            self.j -= 1\n",
    "        elif action == 'L':\n",
    "            self.j += 1\n",
    "        # Raise an exception if we arrive somewhere we shouldn't be\n",
    "        assert(self.current_state() in self.all_states())\n",
    "    \n",
    "    # Returns true if game is over, else false\n",
    "    def game_over(self):\n",
    "        # true if we are in a state where no actions are possible\n",
    "        return (self.i, self.j) not in self.actions\n",
    "    \n",
    "    # Simple way to get all states. ***Possibly buggy***\n",
    "    def all_states(self):\n",
    "        # Either a position that has possible next actions or a position that yields a reward\n",
    "        return set(self.actions.keys()) | set(self.rewards.keys())\n",
    "\n",
    "# Define a grid that describes the reward for arriving at each state and possible actions at each state\n",
    "def standard_grid():\n",
    "    g = Grid(3, 4, (2, 0))\n",
    "    # 'rewards' disctionary has states as keys and consequnces as values\n",
    "    rewards = {(0, 3): 1, (1, 3): -1}\n",
    "    # 'actions' dictionary has states as keys and possible actions as values\n",
    "    actions = {\n",
    "    (0, 0): ('D', 'R'),\n",
    "    (0, 1): ('L', 'R'),\n",
    "    (0, 2): ('L', 'D', 'R'),\n",
    "    (1, 0): ('U', 'D'),\n",
    "    (1, 2): ('U', 'D', 'R'),\n",
    "    (2, 0): ('U', 'R'),\n",
    "    (2, 1): ('L', 'R'),\n",
    "    (2, 2): ('L', 'R', 'U'),\n",
    "    (2, 3): ('L', 'U'),\n",
    "    }\n",
    "    g.set(rewards, actions)\n",
    "    return g\n",
    "\n",
    "# Same as above but has a step cost / penalty for moving\n",
    "def negative_grid(step_cost=-0.1):\n",
    "    g = standard_grid()\n",
    "    g.rewards.update({\n",
    "    (0, 0): step_cost,\n",
    "    (0, 1): step_cost,\n",
    "    (0, 2): step_cost,\n",
    "    (1, 0): step_cost,\n",
    "    (1, 2): step_cost,\n",
    "    (2, 0): step_cost,\n",
    "    (2, 1): step_cost,\n",
    "    (2, 2): step_cost,\n",
    "    (2, 3): step_cost,\n",
    "    })\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0, 1), (1, 2), (0, 0), (1, 3), (2, 1), (2, 0), (2, 3), (2, 2), (1, 0), (0, 2), (0, 3)} \n",
      "\n",
      "{(0, 1): ('L', 'R'), (1, 2): ('U', 'D', 'R'), (0, 0): ('D', 'R'), (2, 3): ('L', 'U'), (2, 0): ('U', 'R'), (1, 0): ('U', 'D'), (2, 2): ('L', 'R', 'U'), (0, 2): ('L', 'D', 'R'), (2, 1): ('L', 'R')} \n",
      "\n",
      "{(0, 3): 1, (1, 3): -1} \n",
      "\n",
      "{(0, 1): -0.1, (1, 2): -0.1, (1, 3): -1, (2, 1): -0.1, (0, 2): -0.1, (2, 0): -0.1, (0, 0): -0.1, (2, 3): -0.1, (2, 2): -0.1, (0, 3): 1, (1, 0): -0.1}\n"
     ]
    }
   ],
   "source": [
    "# Test out the grid\n",
    "g = Grid(width = 3, height = 4, start = (2,0))\n",
    "grid = standard_grid()\n",
    "\n",
    "# Check out all the states, the actions per state, and the possible rewards for the standard grid\n",
    "print(grid.all_states(), '\\n')\n",
    "print(grid.actions, '\\n')\n",
    "print(grid.rewards, '\\n')\n",
    "\n",
    "# All is the same in the negative grid except the 'rewards' for all the neutral states are now -0.1\n",
    "neg_grid = negative_grid()\n",
    "print(neg_grid.rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prints a representation of the map with the values given \n",
    "def print_values(V, g):\n",
    "    for i in range(g.width):\n",
    "        print(\"---------------------------\")\n",
    "        for j in range(g.height):\n",
    "            v = V.get((i,j), 0)\n",
    "            if v >= 0:\n",
    "                print(\" %.2f|\" % v, end=\"\")\n",
    "            else:\n",
    "                print(\"%.2f|\" % v, end=\"\") # -ve sign takes up an extra space\n",
    "        print(\"\")\n",
    "\n",
    "# Prints out map with policy actions given each state\n",
    "def print_policy(P, g):\n",
    "    for i in range(g.width):\n",
    "        print(\"---------------------------\")\n",
    "        for j in range(g.height):\n",
    "            a = P.get((i,j), ' ')\n",
    "            print(\"  %s  |\" % a, end=\"\")\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative Policy Evaluation\n",
    "\n",
    "This method allows an agent to search all possible states and actions by first randomly allowing the agent to choose a policy, evaluate that policy and then update that policy until a certain criterion is met. In this case the criterion will be a threshold of value change. We take the maximum of the difference in value from one state to its next state. If the max value of all the states' changes in value is below 0.001 then we will consider the alrorithm to have converged.\n",
    "\n",
    "NOTE:\n",
    "There are 2 sources of randomness:\n",
    "\n",
    "1) p(a|s) - deciding what action to take given the state\n",
    "\n",
    "2) p(s',r|s,a) - the next state and reward given your action-state pair\n",
    "\n",
    "We are only modeling p(a|s) = uniform. How would the code change if p(s',r|s,a) is not deterministic?\n",
    "\n",
    "The neagtive grid being used gives you a reward of -0.1 for every non-terminal state. We want to see if this will encourage finding a shorter path to the goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards:\n",
      "---------------------------\n",
      "-0.10|-0.10|-0.10| 1.00|\n",
      "---------------------------\n",
      "-0.10| 0.00|-0.10|-1.00|\n",
      "---------------------------\n",
      "-0.10|-0.10|-0.10|-0.10|\n",
      "Initial policy:\n",
      "---------------------------\n",
      "  L  |  U  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  D  |     |\n",
      "---------------------------\n",
      "  D  |  U  |  L  |  R  |\n",
      "Values:\n",
      "---------------------------\n",
      " 0.62| 0.80| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.46| 0.00| 0.80| 0.00|\n",
      "---------------------------\n",
      " 0.31| 0.46| 0.62| 0.46|\n",
      "Policy:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  U  |     |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  L  |\n"
     ]
    }
   ],
   "source": [
    "# Here are some of the hyperparameters we can tweak to get different outputs\n",
    "SMALL_ENOUGH = 1e-3\n",
    "GAMMA = 0.9\n",
    "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    grid = negative_grid()\n",
    "\n",
    "    print(\"Rewards:\")\n",
    "    print_values(grid.rewards, grid)\n",
    "\n",
    "    # We'll randomly choose an action and update as we learn\n",
    "    policy = {}\n",
    "    for s in grid.actions.keys():\n",
    "        policy[s] = np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
    "\n",
    "    print(\"Initial policy:\")\n",
    "    print_policy(policy, grid)\n",
    "\n",
    "    # Initialize V(s)\n",
    "    V = {}\n",
    "    states = grid.all_states()\n",
    "    for s in states:\n",
    "        # V[s] = 0\n",
    "        if s in grid.actions:\n",
    "            V[s] = np.random.random()\n",
    "        else:\n",
    "            # Terminal state\n",
    "            V[s] = 0\n",
    "\n",
    "    # Repeat until convergence - will break out when policy does not change\n",
    "    biggest_change_list = []\n",
    "    while True:\n",
    "        # Policy evaluation step\n",
    "        while True:\n",
    "            biggest_change = 0\n",
    "            for s in states:\n",
    "                old_v = V[s]\n",
    "                # V(s) only has value if it's not a terminal state\n",
    "                if s in policy:\n",
    "                    a = policy[s]\n",
    "                    grid.set_state(s)\n",
    "                    r = grid.move(a)\n",
    "                    V[s] = r + GAMMA * V[grid.current_state()]\n",
    "                    biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
    "            biggest_change_list.append(biggest_change)\n",
    "            if biggest_change < SMALL_ENOUGH:\n",
    "                break\n",
    "    \n",
    "        # Policy improvement step\n",
    "        is_policy_converged = True\n",
    "        for s in states:\n",
    "            if s in policy:\n",
    "                old_a = policy[s]\n",
    "                new_a = None\n",
    "                best_value = float('-inf')\n",
    "                # loop through all possible actions to find the best current action\n",
    "                for a in ALL_POSSIBLE_ACTIONS:\n",
    "                    grid.set_state(s)\n",
    "                    r = grid.move(a)\n",
    "                    v = r + GAMMA * V[grid.current_state()]\n",
    "                    if v > best_value:\n",
    "                        best_value = v\n",
    "                        new_a = a\n",
    "                policy[s] = new_a\n",
    "                if new_a != old_a:\n",
    "                    is_policy_converged = False\n",
    "        if is_policy_converged:\n",
    "            break\n",
    "\n",
    "    print(\"Values:\")\n",
    "    print_values(V, grid)\n",
    "    print(\"Policy:\")\n",
    "    print_policy(policy, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgkAAAFkCAYAAACq4KjhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x157a7e39240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot biggest change\n",
    "import matplotlib.pyplot as plt \n",
    "plt.plot(biggest_change_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Value Iteration\n",
    " \n",
    "Iterating between policies to evaluate and then update them is not the only way to teach an agent to find its way from point A to point B. Instead we can iterate over the values themselves to find the optimal values and THEN update the policy at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # this grid gives you a reward of -0.1 for every non-terminal state\n",
    "    # we want to see if this will encourage finding a shorter path to the goal\n",
    "    grid = negative_grid()\n",
    "\n",
    "    # print rewards\n",
    "    print(\"rewards:\")\n",
    "    print_values(grid.rewards, grid)\n",
    "\n",
    "    # state -> action\n",
    "    # we'll randomly choose an action and update as we learn\n",
    "    policy = {}\n",
    "    for s in grid.actions.keys():\n",
    "        policy[s] = np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
    "\n",
    "    print(\"initial policy:\")\n",
    "    print_policy(policy, grid)\n",
    "\n",
    "    # Initialize V(s)\n",
    "    V = {}\n",
    "    states = grid.all_states()\n",
    "    for s in states:\n",
    "        # V[s] = 0\n",
    "        if s in grid.actions:\n",
    "            V[s] = np.random.random()\n",
    "        else:\n",
    "            # Terminal state\n",
    "            V[s] = 0\n",
    "\n",
    "    # Repeat until convergence\n",
    "    # V[s] = max[a]{ sum[s',r] { p(s',r|s,a)[r + GAMMA*V[s']] } }\n",
    "    count = 0\n",
    "    while True:\n",
    "        count += 1\n",
    "        biggest_change = 0\n",
    "        for s in states:\n",
    "            old_v = V[s]\n",
    "            # V(s) only has value if it's not a terminal state\n",
    "            if s in policy:\n",
    "                new_v = float('-inf')\n",
    "                for a in ALL_POSSIBLE_ACTIONS:\n",
    "                    grid.set_state(s)\n",
    "                    r = grid.move(a)\n",
    "                    v = r + GAMMA * V[grid.current_state()]\n",
    "                    if v > new_v:\n",
    "                        new_v = v\n",
    "                V[s] = new_v\n",
    "                biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
    "\n",
    "        print(count,biggest_change)\n",
    "\n",
    "        if biggest_change < SMALL_ENOUGH:\n",
    "            break\n",
    "\n",
    "    # find a policy that leads to optimal value function\n",
    "    for s in policy.keys():\n",
    "        best_a = None\n",
    "        best_value = float('-inf')\n",
    "        # loop through all possible actions to find the best current action\n",
    "        for a in ALL_POSSIBLE_ACTIONS:\n",
    "            grid.set_state(s)\n",
    "            r = grid.move(a)\n",
    "            v = r + GAMMA * V[grid.current_state()]\n",
    "            if v > best_value:\n",
    "                best_value = v\n",
    "                best_a = a\n",
    "        policy[s] = best_a\n",
    "\n",
    "    # Our goal here is to verify that we get the same answer as with policy iteration\n",
    "    print(\"values:\")\n",
    "    print_values(V, grid)\n",
    "    print(\"policy:\")\n",
    "    print_policy(policy, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOTE: this is only policy evaluation, not optimization\n",
    "\n",
    "def play_game(grid, policy):\n",
    "    # Returns a list of states and corresponding returns\n",
    "\n",
    "    # Reset game to start at a random position\n",
    "    # We need to do this, because given our current deterministic policy\n",
    "    # We would never end up at certain states, but we still want to measure their value\n",
    "    start_states = list(grid.actions.keys())\n",
    "    start_idx = np.random.choice(len(start_states))\n",
    "    grid.set_state(start_states[start_idx])\n",
    "  \n",
    "    s = grid.current_state()\n",
    "    states_and_rewards = [(s, 0)] # list of tuples of (state, reward)\n",
    "    while not grid.game_over():\n",
    "        a = policy[s]\n",
    "        r = grid.move(a)\n",
    "        s = grid.current_state()\n",
    "        states_and_rewards.append((s, r))\n",
    "    # calculate the returns by working backwards from the terminal state\n",
    "    G = 0\n",
    "    states_and_returns = []\n",
    "    first = True\n",
    "    for s, r in reversed(states_and_rewards):\n",
    "        # the value of the terminal state is 0 by definition\n",
    "        # we should ignore the first state we encounter\n",
    "        # and ignore the last G, which is meaningless since it doesn't correspond to any move\n",
    "        if first:\n",
    "            first = False\n",
    "        else:\n",
    "            states_and_returns.append((s, G))\n",
    "        G = r + GAMMA*G\n",
    "    states_and_returns.reverse() # we want it to be in order of state visited\n",
    "    return states_and_returns\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Use the standard grid again (0 for every step) so that we can compare to iterative policy evaluation\n",
    "    grid = standard_grid()\n",
    "\n",
    "    print(\"rewards:\")\n",
    "    print_values(grid.rewards, grid)\n",
    "\n",
    "    # state -> action\n",
    "    policy = {\n",
    "    (2, 0): 'U',\n",
    "    (1, 0): 'U',\n",
    "    (0, 0): 'R',\n",
    "    (0, 1): 'R',\n",
    "    (0, 2): 'R',\n",
    "    (1, 2): 'R',\n",
    "    (2, 1): 'R',\n",
    "    (2, 2): 'U',\n",
    "    (2, 3): 'L',\n",
    "    }\n",
    "\n",
    "    # initialize V(s) and returns\n",
    "    V = {}\n",
    "    returns = {} # dictionary of state -> list of returns we've received\n",
    "    states = grid.all_states()\n",
    "    for s in states:\n",
    "        if s in grid.actions:\n",
    "          returns[s] = []\n",
    "        else:\n",
    "          # terminal state or state we can't otherwise get to\n",
    "          V[s] = 0\n",
    "\n",
    "    # repeat\n",
    "    for t in range(100):\n",
    "        # generate an episode using pi\n",
    "        states_and_returns = play_game(grid, policy)\n",
    "        seen_states = set()\n",
    "        for s, G in states_and_returns:\n",
    "            # check if we have already seen s\n",
    "            # called \"first-visit\" MC policy evaluation\n",
    "            if s not in seen_states:\n",
    "                returns[s].append(G)\n",
    "                V[s] = np.mean(returns[s])\n",
    "                seen_states.add(s)\n",
    "\n",
    "    print(\"values:\")\n",
    "    print_values(V, grid)\n",
    "    print(\"policy:\")\n",
    "    print_policy(policy, grid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources:\n",
    "    \n",
    "https://deeplearningcourses.com/c/artificial-intelligence-reinforcement-learning-in-python\n",
    "    \n",
    "https://www.udemy.com/artificial-intelligence-reinforcement-learning-in-python"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
