{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal Route\n",
    "\n",
    "How can you know the fastest way to get from point A to point B? Well you need to know the distance between points, of course. But what if there are multiple non-linear ways to get there? What then?\n",
    "\n",
    "This is where experience comes in. This experience is usually only gained once you:\n",
    "1) reach the end of the trip and\n",
    "2) have tried multiple trip paths to know the best one.\n",
    "\n",
    "Fortunately, machines can learn the same way! We can provide an agent its environment, tell it where to start, where to end, and where to avoid. The agent will be rewarded only when it reaches its destination. Just like humans don't like waiting in traffic or taking all day to get somewhere, we can discount its final reward by diminishing its reward over time spent getting to its destination. This will \"motivate\" the agent to pick routes that took as little time as possible. \n",
    "\n",
    "I'll set up a deterministic discrete 4 x 3 environment where there are states and actions that occur in turns. This environment will have a negative reward state and a positive reward state, as well as an untoucable / roadblock area. Most of the time (except TD learning) the agent will make a policy of moves it should take only once it reaches a terminal state, which in this case is 1 or -1. Once that maximum reward is found it will look back on which moves it took to get to that point, which we call dynamic programming. \n",
    "\n",
    "With these rules in mind, what would prevent it from just wandering for a long period of time before transitioning to the +1 state? This is where the discounted environment comes in.\n",
    "\n",
    "If we can discount that reward of 1 with -0.1 each time the agent makes a transition between states, then it will want to make as few moves as possible to to maximize its reward. Once it does get to the end, it looks back at the moves it took to get there and calculates the value. So if it took 10 moves, the reward would be 0 (10 moves * -0.1 per move + reward [+1]). Then it tries again. If the second round only took 6 moves, then the reward is 0.4 which is greater. The process repeats until the gain in reward no longer improves. We can also add a hyperparameter called gamma. This is a reward multiplier that can either grow your reward or shink your reward over time depending if its greater than or less than 1. Ours will be set to 0.9\n",
    "\n",
    "After setting up the environment and agent, I will walk us through a history of thinking in reinforcement learning that follows somewhat of an evolution of the subject. We will start with **Policy and Value Evaluation and Iteration**, which looks at all possible states and values, but can become computationally expensive. We will then evolve to less a expensive method called **Monte Carlo Learning**, which updates the policy based of episodes that sample from the state space. Then we will end with state-to-state updates with like **Temporal Difference Learning** and an efficient method called **Q-Learning**.\n",
    "\n",
    "** Disclaimer: Most of this code was taken from Udacity's reinforcement learning course which I have slightly modified for teaching purposes **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note: you may need to update your version of future\n",
    "# sudo pip install -U future\n",
    "from __future__ import print_function, division\n",
    "from builtins import range\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "class Grid: # Environment\n",
    "    def __init__(self, width, height, start):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.i = start[0]\n",
    "        self.j = start[1]\n",
    "    \n",
    "    # This function makes rewards and actions per state into class attributes but is added to the class once they are specified\n",
    "    def set(self, rewards, actions):\n",
    "        # rewards should be a dict of: (i, j): r (row, col): reward\n",
    "        # actions should be a dict of: (i, j): A (row, col): list of possible actions\n",
    "        self.rewards = rewards\n",
    "        self.actions = actions\n",
    "    \n",
    "    # Takes a tuple of a state and assigns each x and y index to i and j\n",
    "    def set_state(self, s):\n",
    "        self.i = s[0]\n",
    "        self.j = s[1]\n",
    "    \n",
    "    # Returns tuple of current state\n",
    "    def current_state(self):\n",
    "        return (self.i, self.j)\n",
    "    \n",
    "    # Checks if agent has reached a terminal state\n",
    "    def is_terminal(self, s):\n",
    "        return s not in self.actions\n",
    "    \n",
    "    # How the agent interacts with the environment\n",
    "    def move(self, action):\n",
    "        # Check if legal move first\n",
    "        if action in self.actions[(self.i, self.j)]:\n",
    "            if action == 'U':\n",
    "                self.i -= 1\n",
    "            elif action == 'D':\n",
    "                self.i += 1\n",
    "            elif action == 'R':\n",
    "                self.j += 1\n",
    "            elif action == 'L':\n",
    "                self.j -= 1\n",
    "        # Return a reward from that state if any. If none, return 0\n",
    "        return self.rewards.get((self.i, self.j), 0)\n",
    "    \n",
    "    # Used to backtrack as the optimal policy is calculated\n",
    "    def undo_move(self, action):\n",
    "        # These are the opposite of what U/D/L/R should normally do\n",
    "        if action == 'U':\n",
    "            self.i += 1\n",
    "        elif action == 'D':\n",
    "            self.i -= 1\n",
    "        elif action == 'R':\n",
    "            self.j -= 1\n",
    "        elif action == 'L':\n",
    "            self.j += 1\n",
    "        # Raise an exception if we arrive somewhere we shouldn't be\n",
    "        assert(self.current_state() in self.all_states())\n",
    "    \n",
    "    # Returns true if game is over, else false\n",
    "    def game_over(self):\n",
    "        # true if we are in a state where no actions are possible\n",
    "        return (self.i, self.j) not in self.actions\n",
    "    \n",
    "    # Simple way to get all states. ***Possibly buggy***\n",
    "    def all_states(self):\n",
    "        # Either a position that has possible next actions or a position that yields a reward\n",
    "        return set(self.actions.keys()) | set(self.rewards.keys())\n",
    "\n",
    "# Define a grid that describes the reward for arriving at each state and possible actions at each state\n",
    "def standard_grid():\n",
    "    g = Grid(3, 4, (2, 0))\n",
    "    # 'rewards' disctionary has states as keys and consequnces as values\n",
    "    rewards = {(0, 3): 1, (1, 3): -1}\n",
    "    # 'actions' dictionary has states as keys and possible actions as values\n",
    "    actions = {\n",
    "    (0, 0): ('D', 'R'),\n",
    "    (0, 1): ('L', 'R'),\n",
    "    (0, 2): ('L', 'D', 'R'),\n",
    "    (1, 0): ('U', 'D'),\n",
    "    (1, 2): ('U', 'D', 'R'),\n",
    "    (2, 0): ('U', 'R'),\n",
    "    (2, 1): ('L', 'R'),\n",
    "    (2, 2): ('L', 'R', 'U'),\n",
    "    (2, 3): ('L', 'U'),\n",
    "    }\n",
    "    g.set(rewards, actions)\n",
    "    return g\n",
    "\n",
    "# Same as above but has a step cost / penalty for moving\n",
    "def negative_grid(step_cost=-0.1):\n",
    "    g = standard_grid()\n",
    "    g.rewards.update({\n",
    "    (0, 0): step_cost,\n",
    "    (0, 1): step_cost,\n",
    "    (0, 2): step_cost,\n",
    "    (1, 0): step_cost,\n",
    "    (1, 2): step_cost,\n",
    "    (2, 0): step_cost,\n",
    "    (2, 1): step_cost,\n",
    "    (2, 2): step_cost,\n",
    "    (2, 3): step_cost,\n",
    "    })\n",
    "    return g\n",
    "\n",
    "# Prints a representation of the map with the values given \n",
    "def print_values(V, g):\n",
    "    for i in range(g.width):\n",
    "        print(\"---------------------------\")\n",
    "        for j in range(g.height):\n",
    "            v = V.get((i,j), 0)\n",
    "            if v >= 0:\n",
    "                print(\" %.2f|\" % v, end=\"\")\n",
    "            else:\n",
    "                print(\"%.2f|\" % v, end=\"\") # -ve sign takes up an extra space\n",
    "        print(\"\")\n",
    "\n",
    "# Prints out map with policy actions given each state\n",
    "def print_policy(P, g):\n",
    "    for i in range(g.width):\n",
    "        print(\"---------------------------\")\n",
    "        for j in range(g.height):\n",
    "            a = P.get((i,j), ' ')\n",
    "            print(\"  %s  |\" % a, end=\"\")\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All States: \n",
      " {(0, 1), (1, 2), (0, 0), (1, 3), (2, 1), (2, 0), (2, 3), (2, 2), (1, 0), (0, 2), (0, 3)} \n",
      "\n",
      "All Actions: \n",
      " {(0, 1): ('L', 'R'), (1, 2): ('U', 'D', 'R'), (0, 0): ('D', 'R'), (2, 3): ('L', 'U'), (2, 0): ('U', 'R'), (1, 0): ('U', 'D'), (2, 2): ('L', 'R', 'U'), (0, 2): ('L', 'D', 'R'), (2, 1): ('L', 'R')} \n",
      "\n",
      "All Standard-grid Rewards: \n",
      " {(0, 3): 1, (1, 3): -1} \n",
      "\n",
      "All Negative-grid Rewards: \n",
      " {(0, 1): -0.1, (1, 2): -0.1, (1, 3): -1, (2, 1): -0.1, (0, 2): -0.1, (2, 0): -0.1, (0, 0): -0.1, (2, 3): -0.1, (2, 2): -0.1, (0, 3): 1, (1, 0): -0.1}\n"
     ]
    }
   ],
   "source": [
    "# Test out the grid\n",
    "g = Grid(width = 3, height = 4, start = (2,0))\n",
    "grid = standard_grid()\n",
    "\n",
    "# Check out all the states, the actions per state, and the possible rewards for the standard grid\n",
    "print('All States:', '\\n', grid.all_states(), '\\n')\n",
    "print('All Actions:', '\\n', grid.actions, '\\n')\n",
    "print('All Standard-grid Rewards:', '\\n', grid.rewards, '\\n')\n",
    "\n",
    "# All is the same in the negative grid except the 'rewards' for all the neutral states are now -0.1\n",
    "neg_grid = negative_grid()\n",
    "print('All Negative-grid Rewards:', '\\n',neg_grid.rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative Policy Evaluation\n",
    "\n",
    "This method allows an agent to search all possible states and actions by first randomly allowing the agent to choose a policy, evaluate that policy and then update that policy until a certain criterion is met. In this case the criterion will be a threshold of value change. We take the maximum of the difference in value across all states from one epoch/iteration to the next. If the max value of all the states' changes in value is below 0.001 then we will consider the algorithm to have converged.\n",
    "\n",
    "NOTE:\n",
    "There are 2 sources of randomness:\n",
    "\n",
    "1) p(action|state) - deciding what action to take given the state\n",
    "\n",
    "2) p(next_state', reward|state, action) - the next state and reward given your action-state pair\n",
    "\n",
    "Since the negative grid is being used it gives you a reward of -0.1 for transitioning between every non-terminal state. This grid will be used throughout this walkthrough. This will encourage finding a shorter path to the goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards:\n",
      "---------------------------\n",
      "-0.10|-0.10|-0.10| 1.00|\n",
      "---------------------------\n",
      "-0.10| 0.00|-0.10|-1.00|\n",
      "---------------------------\n",
      "-0.10|-0.10|-0.10|-0.10|\n",
      "\n",
      " Initial policy:\n",
      "---------------------------\n",
      "  L  |  D  |  R  |     |\n",
      "---------------------------\n",
      "  R  |     |  L  |     |\n",
      "---------------------------\n",
      "  D  |  D  |  U  |  L  |\n",
      "\n",
      " Values:\n",
      "---------------------------\n",
      " 0.62| 0.80| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.46| 0.00| 0.80| 0.00|\n",
      "---------------------------\n",
      " 0.31| 0.46| 0.62| 0.46|\n",
      "\n",
      " Policy:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  U  |     |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  L  |\n"
     ]
    }
   ],
   "source": [
    "# Here are some of the hyperparameters we can tweak to get different outputs\n",
    "SMALL_ENOUGH = 1e-3\n",
    "GAMMA = 0.9\n",
    "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    grid = negative_grid()\n",
    "\n",
    "    print('Rewards:')\n",
    "    print_values(grid.rewards, grid)\n",
    "\n",
    "    # We'll randomly choose an action and update as we learn\n",
    "    policy = {}\n",
    "    for s in grid.actions.keys():\n",
    "        policy[s] = np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
    "\n",
    "    print('\\n', 'Initial policy:')\n",
    "    print_policy(policy, grid)\n",
    "\n",
    "    # Initialize V(s)\n",
    "    V = {}\n",
    "    states = grid.all_states()\n",
    "    for s in states:\n",
    "        # V[s] = 0\n",
    "        if s in grid.actions:\n",
    "            V[s] = np.random.random()\n",
    "        else:\n",
    "            # Terminal state\n",
    "            V[s] = 0\n",
    "            \n",
    "    # Record biggest change within state for each iteration\n",
    "    biggest_change_list = []\n",
    "    \n",
    "    # Repeat until convergence - will break out when policy does not change\n",
    "    while True:\n",
    "        # Policy evaluation step\n",
    "        while True:\n",
    "            biggest_change = 0\n",
    "            for s in states:\n",
    "                old_v = V[s]\n",
    "                # V(s) only has value if it's not a terminal state\n",
    "                if s in policy:\n",
    "                    a = policy[s]\n",
    "                    grid.set_state(s)\n",
    "                    r = grid.move(a)\n",
    "                    V[s] = r + GAMMA * V[grid.current_state()]\n",
    "                    biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
    "            biggest_change_list.append(biggest_change)\n",
    "            if biggest_change < SMALL_ENOUGH:\n",
    "                break\n",
    "    \n",
    "        # Policy improvement step\n",
    "        is_policy_converged = True\n",
    "        for s in states:\n",
    "            if s in policy:\n",
    "                old_a = policy[s]\n",
    "                new_a = None\n",
    "                best_value = float('-inf')\n",
    "                # loop through all possible actions to find the best current action\n",
    "                for a in ALL_POSSIBLE_ACTIONS:\n",
    "                    grid.set_state(s)\n",
    "                    r = grid.move(a)\n",
    "                    v = r + GAMMA * V[grid.current_state()]\n",
    "                    if v > best_value:\n",
    "                        best_value = v\n",
    "                        new_a = a\n",
    "                policy[s] = new_a\n",
    "                if new_a != old_a:\n",
    "                    is_policy_converged = False\n",
    "        if is_policy_converged:\n",
    "            break\n",
    "\n",
    "    print('\\n', 'Values:')\n",
    "    print_values(V, grid)\n",
    "    print('\\n', 'Policy:')\n",
    "    print_policy(policy, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgkAAAFkCAYAAACq4KjhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkCBJkhoyJEiSpIYMCZIkqaH/H8xPLTPVXXaeAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b007ad4438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot biggest change\n",
    "plt.plot(biggest_change_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The learning occurred with small changes in maximum deltas over iterations until about later epochs where a large change in at least one of the states occurred. Due to the algorithm's properties, we are almost certain that this was an improvement from around -1 to around 1, thus giving a difference of almost 2. Then for each iteration the adjascent states likely changed as a result, resulting in 2 more spikes of learning until convergence. \n",
    "\n",
    "If you compare the initial random policy to the final policy, you can see how much it improved through this process. Comparitively this process took a while to do so converge. Fortunately there are better ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Value Iteration\n",
    " \n",
    "As mentioned, iterating between policies to evaluate and then update them is not the only way to teach an agent to find its way from point A to point B. Instead we can iterate over the values themselves to find the optimal values and THEN update the policy to give those values at the end. If we try to change the policy after every litte shift in value it will usually take longer. This also requires a bit less code (notice the outer 'while' loop is now removed), which makes this method a win-win for both agent and programmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Initial policy:\n",
      "---------------------------\n",
      "  R  |  D  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  U  |     |\n",
      "---------------------------\n",
      "  U  |  D  |  R  |  L  |\n",
      "\n",
      "Biggest Value Change per Iteration \n",
      " 1 0.5923902660856444\n",
      "Biggest Value Change per Iteration \n",
      " 2 0.15749326716258094\n",
      "Biggest Value Change per Iteration \n",
      " 3 0.10523940446322833\n",
      "Biggest Value Change per Iteration \n",
      " 4 0\n",
      "\n",
      " Values:\n",
      "---------------------------\n",
      " 0.62| 0.80| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.46| 0.00| 0.80| 0.00|\n",
      "---------------------------\n",
      " 0.31| 0.46| 0.62| 0.46|\n",
      "\n",
      " Policy:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  U  |     |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  L  |\n"
     ]
    }
   ],
   "source": [
    "SMALL_ENOUGH = 1e-3\n",
    "GAMMA = 0.9\n",
    "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # this grid gives you a reward of -0.1 for every non-terminal state\n",
    "    # we want to see if this will encourage finding a shorter path to the goal\n",
    "    grid = negative_grid()\n",
    "    # state -> action\n",
    "    # we'll randomly choose an action and update as we learn\n",
    "    policy = {}\n",
    "    for s in grid.actions.keys():\n",
    "        policy[s] = np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
    "\n",
    "    print('\\n', 'Initial policy:')\n",
    "    print_policy(policy, grid)\n",
    "    print()\n",
    "\n",
    "    # Initialize V(s)\n",
    "    V = {}\n",
    "    states = grid.all_states()\n",
    "    for s in states:\n",
    "        # V[s] = 0\n",
    "        if s in grid.actions:\n",
    "            V[s] = np.random.random()\n",
    "        else:\n",
    "            # Terminal state\n",
    "            V[s] = 0\n",
    "\n",
    "    # Repeat until convergence\n",
    "    # V[s] = max[a]{ sum[s',r] { p(s',r|s,a)[r + GAMMA*V[s']] } }\n",
    "    count = 0\n",
    "    while True:\n",
    "        count += 1\n",
    "        biggest_change = 0\n",
    "        for s in states:\n",
    "            old_v = V[s]\n",
    "            # V(s) only has value if it's not a terminal state\n",
    "            if s in policy:\n",
    "                new_v = float('-inf')\n",
    "                for a in ALL_POSSIBLE_ACTIONS:\n",
    "                    grid.set_state(s)\n",
    "                    r = grid.move(a)\n",
    "                    v = r + GAMMA * V[grid.current_state()]\n",
    "                    if v > new_v:\n",
    "                        new_v = v\n",
    "                V[s] = new_v\n",
    "                biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
    "\n",
    "        print('Biggest Value Change per Iteration', '\\n', count,biggest_change)\n",
    "\n",
    "        if biggest_change < SMALL_ENOUGH:\n",
    "            break\n",
    "\n",
    "    # find a policy that leads to optimal value function\n",
    "    for s in policy.keys():\n",
    "        best_a = None\n",
    "        best_value = float('-inf')\n",
    "        # loop through all possible actions to find the best current action\n",
    "        for a in ALL_POSSIBLE_ACTIONS:\n",
    "            grid.set_state(s)\n",
    "            r = grid.move(a)\n",
    "            v = r + GAMMA * V[grid.current_state()]\n",
    "            if v > best_value:\n",
    "                best_value = v\n",
    "                best_a = a\n",
    "        policy[s] = best_a\n",
    "\n",
    "    # Our goal here is to verify that we get the same answer as with policy iteration\n",
    "    print('\\n', 'Values:')\n",
    "    print_values(V, grid)\n",
    "    print('\\n', 'Policy:')\n",
    "    print_policy(policy, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method only took 4 iterations! In a way this method is much more of a \"think before you act\" method than policy iteration is. However there are many environments that will not be able to iterated over every state and action to get a value. Some environments have too many states to hold in memory. In these situations we must code in a way closer to how humans think. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Method\n",
    "\n",
    "We will still use the same environment for simplicity's sake, but instead of iterating through every state and action, we will pick random actions at each state until a terminal state is met. We will then calculate the value for each state-action pair in that path and repeat the process, appending a value to each state-action the agent is a part of. The list of values each state accumulates is then averaged to get an estimate for that states value, given an action. This process continues until we say so, or more accurately, until we think an optimal policy has been found. In large environments however, we would not be sure this was an optimal policy globally, but would have a level of confidence based off the end result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Episodes\n",
      "1000 Episodes\n",
      "2000 Episodes\n",
      "3000 Episodes\n",
      "4000 Episodes\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhIAAAFkCAYAAAB1rtL+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkCRJ2QwSkiQp2/8DXQ6fr2i+bBAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b019863588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Final values:\n",
      "---------------------------\n",
      " 0.58| 0.77| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.41| 0.00| 0.77| 0.00|\n",
      "---------------------------\n",
      " 0.25| 0.09| 0.00|-0.39|\n",
      "\n",
      " Final policy:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  U  |     |\n",
      "---------------------------\n",
      "  U  |  L  |  L  |  L  |\n"
     ]
    }
   ],
   "source": [
    "# NOTE: find optimal policy and value function using on-policy first-visit MC\n",
    "GAMMA = 0.9\n",
    "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')\n",
    "\n",
    "# Returns the argmax (key) and max (value) from a dictionary\n",
    "def max_dict(d):\n",
    "    max_key = None\n",
    "    max_val = float('-inf')\n",
    "    for k, v in d.items():\n",
    "        if v > max_val:\n",
    "            max_val = v\n",
    "            max_key = k\n",
    "    return max_key, max_val\n",
    "\n",
    "def random_action(a, eps=0.1):\n",
    "    p = np.random.random()\n",
    "    if p < (1 - eps):\n",
    "        return a\n",
    "    else:\n",
    "        return np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
    "\n",
    "# In this version we will NOT use \"exploring starts\" method. Instead we will explore using an epsilon-soft policy\n",
    "# Returns a list of states and corresponding returns\n",
    "def play_game(grid, policy):\n",
    "    s = (2, 0)\n",
    "    grid.set_state(s)\n",
    "    a = random_action(policy[s])\n",
    "    # Each triple is s(t), a(t), r(t) but r(t) results from taking action a(t-1) from s(t-1) and landing in s(t)\n",
    "    states_actions_rewards = [(s, a, 0)]\n",
    "    while True:\n",
    "        r = grid.move(a)\n",
    "        s = grid.current_state()\n",
    "        if grid.game_over():\n",
    "            states_actions_rewards.append((s, None, r))\n",
    "            break\n",
    "        else:\n",
    "            a = random_action(policy[s]) # The next state is stochastic\n",
    "            states_actions_rewards.append((s, a, r))\n",
    "\n",
    "    # Calculate the returns by working backwards from the terminal state\n",
    "    G = 0\n",
    "    states_actions_returns = []\n",
    "    first = True\n",
    "    for s, a, r in reversed(states_actions_rewards):\n",
    "        # We should ignore the first state we encounter and the last G, which is meaningless since it doesn't correspond to any move\n",
    "        if first:\n",
    "            first = False\n",
    "        else:\n",
    "            states_actions_returns.append((s, a, G))\n",
    "        G = r + GAMMA*G\n",
    "    states_actions_returns.reverse() # We want it to be in order of state visited\n",
    "    return states_actions_returns\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    grid = negative_grid(step_cost=-0.1)\n",
    "    policy = {}\n",
    "    for s in grid.actions.keys():\n",
    "        policy[s] = np.random.choice(ALL_POSSIBLE_ACTIONS)\n",
    "\n",
    "    Q = {}\n",
    "    returns = {} # Dictionary of state -> list of returns we've received\n",
    "    states = grid.all_states()\n",
    "    for s in states:\n",
    "        if s in grid.actions:\n",
    "            Q[s] = {}\n",
    "            for a in ALL_POSSIBLE_ACTIONS:\n",
    "                Q[s][a] = 0\n",
    "                returns[(s,a)] = []\n",
    "        else:\n",
    "            # Terminal state or state we can't otherwise get to\n",
    "            pass\n",
    "\n",
    "    # Repeat until convergence\n",
    "    deltas = []\n",
    "    for t in range(5000):\n",
    "        if t % 1000 == 0:\n",
    "            print(t, 'Episodes')\n",
    "\n",
    "        biggest_change = 0\n",
    "        states_actions_returns = play_game(grid, policy)\n",
    "\n",
    "        # Calculate Q(s,a)\n",
    "        seen_state_action_pairs = set()\n",
    "        for s, a, G in states_actions_returns:\n",
    "            # Check if we have already seen s. Called \"first-visit\" MC policy evaluation\n",
    "            sa = (s, a)\n",
    "            if sa not in seen_state_action_pairs:\n",
    "                old_q = Q[s][a]\n",
    "                returns[sa].append(G)\n",
    "                Q[s][a] = np.mean(returns[sa])\n",
    "                biggest_change = max(biggest_change, np.abs(old_q - Q[s][a]))\n",
    "                seen_state_action_pairs.add(sa)\n",
    "        deltas.append(biggest_change)\n",
    "\n",
    "        # Calculate new policy\n",
    "        for s in policy.keys():\n",
    "            a, _ = max_dict(Q[s])\n",
    "            policy[s] = a\n",
    "\n",
    "    plt.plot(deltas)\n",
    "    plt.show()\n",
    "\n",
    "    # Find the optimal state-value function\n",
    "    V = {}\n",
    "    for s in policy.keys():\n",
    "        V[s] = max_dict(Q[s])[1]\n",
    "\n",
    "    print('\\n', 'Final values:')\n",
    "    print_values(V, grid)\n",
    "    print('\\n', 'Final policy:')\n",
    "    print_policy(policy, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though we have not iterated through each state and its actions to know its value for sure, after 5000 rounds of playing the game we were able to estimate each state's value. With these values we can update the policy to get an estimated policy. In larger environments this method is much more tractable and allows for more accurate estimates of an optimal policy as it iterates to a terminal state / threshold of rewards.\n",
    "\n",
    "Do notice the list of deltas, which is similar to the plot under Iterative Policy Evaluation. It shows the max difference in values as policies were changed. Roughly speaking, these spikes in differences are mostly (if not completely) differences that maximize state-action values, making them moments where the algorithm learned. You can see the magnitude of learning in the max state-action value difference per episode. In humans, this is perhaps known as an 'a-ha' moment. Mathematically it is different, but the general idea draws similarities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal Difference Learning\n",
    "\n",
    "With Monte Carlo methods \"learning\" can only occur at the end of each episode. Temporal difference learning allows a model to learn while still within an episode. From state to state the value is estimated which I will display below using a fixed policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second Episode Values: \n",
      " [-0.0199, -0.0199, -0.0199, -0.020791000000000004, -0.029701000000000005, -0.03058309, -0.03948337810000001, -0.041078285029, -0.04740623029000001, -0.019711900000000004, 0.19] \n",
      "\n",
      "Last Episode Values: \n",
      " [0.2429407893009413, 0.41189915071709304, 0.5707748744055352, 0.7684182047350576, 0.9910700220136391] \n",
      "\n",
      "Policy:\n",
      "---------------------------\n",
      "  R  |  R  |  R  |     |\n",
      "---------------------------\n",
      "  U  |     |  U  |     |\n",
      "---------------------------\n",
      "  U  |  R  |  U  |  L  |\n"
     ]
    }
   ],
   "source": [
    "SMALL_ENOUGH = 1e-3\n",
    "GAMMA = 0.9\n",
    "ALPHA = 0.1\n",
    "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')\n",
    "\n",
    "# NOTE: this is only policy evaluation, not optimization\n",
    "\n",
    "# Returns a list of states and corresponding rewards (not returns as in MC)\n",
    "def play_game(grid, policy):\n",
    "    # Start at the designated start state\n",
    "    s = (2, 0)\n",
    "    grid.set_state(s)\n",
    "    states_and_rewards = [(s, 0)] # list of tuples of (state, reward)\n",
    "    while not grid.game_over():\n",
    "        a = policy[s]\n",
    "        a = random_action(a)\n",
    "        r = grid.move(a)\n",
    "        s = grid.current_state()\n",
    "        states_and_rewards.append((s, r))\n",
    "    return states_and_rewards\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    grid = negative_grid()\n",
    "\n",
    "    # state -> action\n",
    "    policy = {\n",
    "    (2, 0): 'U',\n",
    "    (1, 0): 'U',\n",
    "    (0, 0): 'R',\n",
    "    (0, 1): 'R',\n",
    "    (0, 2): 'R',\n",
    "    (1, 2): 'U',\n",
    "    (2, 1): 'R',\n",
    "    (2, 2): 'U',\n",
    "    (2, 3): 'L',\n",
    "    }\n",
    "\n",
    "    V = {}\n",
    "    V_list_first = []\n",
    "    V_list_last = []\n",
    "    states = grid.all_states()\n",
    "    for s in states:\n",
    "        V[s] = 0\n",
    "\n",
    "    # Repeat until convergence\n",
    "    for it in range(1000):\n",
    "        # generate an episode using pi\n",
    "        states_and_rewards = play_game(grid, policy)\n",
    "        # the first (s, r) tuple is the state we start in and 0\n",
    "        # (since we don't get a reward) for simply starting the game\n",
    "        # the last (s, r) tuple is the terminal state and the final reward\n",
    "        # the value for the terminal state is by definition 0, so we don't\n",
    "        # care about updating it.\n",
    "        for t in range(len(states_and_rewards) - 1):\n",
    "            s, _ = states_and_rewards[t]\n",
    "            s2, r = states_and_rewards[t+1]\n",
    "            # We will update V(s) as we experience the episode\n",
    "            V[s] = V[s] + ALPHA*(r + GAMMA*V[s2] - V[s])\n",
    "            if it == 1:\n",
    "                V_list_first.append(V[s])\n",
    "            if it == 999:\n",
    "                V_list_last.append(V[s])\n",
    "                \n",
    "    print('Second Episode Values:', '\\n', V_list_first, '\\n')\n",
    "    print('Last Episode Values:', '\\n', V_list_last, '\\n')\n",
    "    print(\"Policy:\")\n",
    "    print_policy(policy, grid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am displaying the learning that took place in the second and last values. I'm using the second episode because it is going off the reward at the end of the first episode. We can also see the values of the last episode to see if there was improvement after many episodes. Within each episode the current value is calculated using each following value in a chain of value evaluation which makes for more dynamic environment searching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning\n",
    "\n",
    "This is perhaps the most widly used and successful method of reinforcement learning. It has a similar feel to Monte Carlo learning in that it does not have to explore the entire environment, however it's different in that it doesnt average returns of explored states. In Q-learning an agent is able to explore state-action pairs without actually saving them / changing the policy, making this an off-policy reinforcement method. The agent will only update its state-action value (known as Q) if it exceeds the current state-action value. This allows for very quick convergence in environments that cannot be fully explored. It also allows for better management of memory since less optimal exploration is not saved.\n",
    "\n",
    "One other thing that makes Q-learning different is the hyperparameter alpha. This is a learning rate that determines what proportion of the max value update should be used to add to the current value, which allows the algorithm to take larger or smaller steps toward the direction it's learning. The method coded below takes it one step further and decreases this learning rate by 0.005 each iteration so that the agent takes smaller and smaller steps toward convergence.\n",
    "\n",
    "Let's see how it learns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhsAAAFkCAYAAACJu/k0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b019838470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "GAMMA = 0.9\n",
    "ALPHA = 0.1\n",
    "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    grid = negative_grid()\n",
    "\n",
    "    # No policy initialization, we will derive our policy from most recent Q. Initialize Q(s,a)\n",
    "    Q = {}\n",
    "    states = grid.all_states()\n",
    "    for s in states:\n",
    "        Q[s] = {}\n",
    "        for a in ALL_POSSIBLE_ACTIONS:\n",
    "            Q[s][a] = 0\n",
    "\n",
    "    # Let's also keep track of how many times Q[s] has been updated\n",
    "    update_counts = {}\n",
    "    update_counts_sa = {}\n",
    "    for s in states:\n",
    "        update_counts_sa[s] = {}\n",
    "        for a in ALL_POSSIBLE_ACTIONS:\n",
    "            update_counts_sa[s][a] = 1.0\n",
    "\n",
    "    t = 1.0\n",
    "    deltas = []\n",
    "    for it in range(1000):\n",
    "        if it % 100 == 0:\n",
    "            t += 1e-2\n",
    "        if it % 2000 == 0:\n",
    "            print(\"it:\", it)\n",
    "        # Instead of 'generating' an epsiode, we will PLAY an episode within this loop\n",
    "        s = (2, 0) # start state\n",
    "        grid.set_state(s)\n",
    "\n",
    "        # The first (s, r) tuple is the state we start in and has a value of 0\n",
    "        # The last (s, r) tuple is the terminal state which has the value 0, so we don't care about updating it.\n",
    "        a, _ = max_dict(Q[s])\n",
    "        biggest_change = 0\n",
    "        while not grid.game_over():\n",
    "            # Random action also works, but is slower than np.random.choice since you can bump into walls\n",
    "            a = random_action(a, eps=0.5/t) # epsilon-greedy\n",
    "            r = grid.move(a)\n",
    "            s2 = grid.current_state()\n",
    "\n",
    "            # Adaptive learning rate\n",
    "            alpha = ALPHA / update_counts_sa[s][a]\n",
    "            update_counts_sa[s][a] += 0.005\n",
    "\n",
    "            # We will update Q(s,a) as we experience the episode\n",
    "            old_qsa = Q[s][a]\n",
    "            a2, max_q_s2a2 = max_dict(Q[s2])\n",
    "            Q[s][a] = Q[s][a] + alpha*(r + GAMMA*max_q_s2a2 - Q[s][a])\n",
    "            biggest_change = max(biggest_change, np.abs(old_qsa - Q[s][a]))\n",
    "\n",
    "            # We would like to know how often Q(s) has been updated too\n",
    "            update_counts[s] = update_counts.get(s,0) + 1\n",
    "\n",
    "            # Next state becomes current state\n",
    "            s = s2\n",
    "\n",
    "        deltas.append(biggest_change)\n",
    "\n",
    "    plt.plot(deltas)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This trend is remarkably different from the previous learning rates. Before there were sporadic jumps in the biggest changes in the environment, but with the adaptive learning rate and Q-learning algorithm we see huge spikes in learning. These quickly become smaller and smaller improvements which allows for steady convergence. If you would like the values to converge more quickly the alpha value can be increased so that maximum state-action values can have bigger influence per iteration. After a shorter time in this environment the values would stop changing so much and convergence would be found. In more varied environments a smaller learning rate may be optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Summary\n",
    "\n",
    "Well, lots of ground was covered here. We began with ideas rooted purely in dynamic programming which evolved into sampling / exploratory approaches that can be applied to larger environments. We finally ended where most reinforcement learning practicianors likely spend their time, in Q-learning.\n",
    "\n",
    "Hopefully this guide can help you explore your own more advanced environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources:\n",
    "    \n",
    "https://deeplearningcourses.com/c/artificial-intelligence-reinforcement-learning-in-python\n",
    "    \n",
    "https://www.udemy.com/artificial-intelligence-reinforcement-learning-in-python"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
